{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ambient-boxing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "from flask import Flask, render_template, request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "stuffed-tissue",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PostID</th>\n",
       "      <th>Post Title</th>\n",
       "      <th>Post Description</th>\n",
       "      <th>Product ID</th>\n",
       "      <th>Brand Name</th>\n",
       "      <th>Data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.165217e+13</td>\n",
       "      <td>Ustraa Black Deodrant Body Spray</td>\n",
       "      <td>Ustraa Black Deodrant Body Spray%0AUstraa Blac...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ustraa black deodrant body spray ustraa black ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.165217e+13</td>\n",
       "      <td>Bella Vita Organic Unisex Luxury Perfume Gift</td>\n",
       "      <td>Bella Vita Organic Unisex Luxury Perfume Gift%...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bella vita organic unisex luxury perfume gift ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.165217e+13</td>\n",
       "      <td>Globus Naturals Pimple Clear Glycolic Acid Fac...</td>\n",
       "      <td>Globus Naturals Pimple Clear Glycolic Acid Fac...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>globus naturals pimple clear glycolic acid fac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.165217e+13</td>\n",
       "      <td>Organic B Handmade Neem Wood Comb</td>\n",
       "      <td>Organic B Handmade Neem Wood Comb%0AOrganic B'...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>organic b handmade neem wood comb organic b ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.165216e+13</td>\n",
       "      <td>Note Cosmetique Terracotta Blusher</td>\n",
       "      <td>Note Cosmetique Terracotta Blusher%0AFor those...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>note cosmetique terracotta blusher note cosmet...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         PostID                                         Post Title  \\\n",
       "0  2.165217e+13                   Ustraa Black Deodrant Body Spray   \n",
       "1  3.165217e+13      Bella Vita Organic Unisex Luxury Perfume Gift   \n",
       "2  6.165217e+13  Globus Naturals Pimple Clear Glycolic Acid Fac...   \n",
       "3  2.165217e+13                  Organic B Handmade Neem Wood Comb   \n",
       "4  5.165216e+13                 Note Cosmetique Terracotta Blusher   \n",
       "\n",
       "                                    Post Description  Product ID Brand Name  \\\n",
       "0  Ustraa Black Deodrant Body Spray%0AUstraa Blac...         NaN        NaN   \n",
       "1  Bella Vita Organic Unisex Luxury Perfume Gift%...         NaN        NaN   \n",
       "2  Globus Naturals Pimple Clear Glycolic Acid Fac...         NaN        NaN   \n",
       "3  Organic B Handmade Neem Wood Comb%0AOrganic B'...         NaN        NaN   \n",
       "4  Note Cosmetique Terracotta Blusher%0AFor those...         NaN        NaN   \n",
       "\n",
       "                                                Data  \n",
       "0  ustraa black deodrant body spray ustraa black ...  \n",
       "1  bella vita organic unisex luxury perfume gift ...  \n",
       "2  globus naturals pimple clear glycolic acid fac...  \n",
       "3  organic b handmade neem wood comb organic b ha...  \n",
       "4  note cosmetique terracotta blusher note cosmet...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_1_df=pd.read_csv('C://Users/USER/Downloads/post_1_df_id.csv')\n",
    "# post_1_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "incorrect-newman",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_1_df.drop(columns=['Product ID','Brand Name'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "backed-session",
   "metadata": {},
   "outputs": [],
   "source": [
    "# post_1_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "impaired-heater",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_1_df.fillna(\" \",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "innovative-remedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# post_1_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "mature-frost",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "stop_words = stopwords.words(\"english\")\n",
    "punc = string.punctuation\n",
    "spec_chars = [\"\\\\n\",\"\\\\xa0\",\"!\",'\"',\"#\",\"%\",\"&\",\"'\",\"(\",\")\",\n",
    "              \"*\",\"+\",\",\",\"-\",\".\",\"/\",\":\",\";\",\"<\",\n",
    "              \"=\",\">\",\"?\",\"@\",\"[\",\"\\\\\",\"]\",\"^\",\"_\",\n",
    "              \"`\",\"{\",\"|\",\"}\",\"~\",\"â€“\",\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "prerequisite-leather",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_WORDS = 4\n",
    "MAX_WORDS = 200\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "def tokenizer(sentence, min_words=MIN_WORDS, max_words=MAX_WORDS,lemmatize=True):\n",
    "    \"\"\"\n",
    "    Lemmatize, tokenize, crop and remove stop words.\n",
    "    \"\"\"\n",
    "    if lemmatize:\n",
    "        stemmer = WordNetLemmatizer()\n",
    "        tokens = [stemmer.lemmatize(w) for w in word_tokenize(sentence)]\n",
    "    else:\n",
    "        tokens = [w for w in word_tokenize(sentence)]\n",
    "    token = [w for w in tokens if (len(w) > min_words and len(w) < max_words\n",
    "                                                        and w not in stop_words)]\n",
    "    return tokens\n",
    "# post_1_df.head()\n",
    "\n",
    "def extract_best_indices(m, topk, mask=None):\n",
    "    \"\"\"\n",
    "    Use sum of the cosine distance over all tokens.\n",
    "    m (np.array): cos matrix of shape (nb_in_tokens, nb_dict_tokens)\n",
    "    topk (int): number of indices to return (from high to lowest in order)\n",
    "    \"\"\"\n",
    "    # return the sum on all tokens of cosinus for each sentence\n",
    "    if len(m.shape) > 1:\n",
    "        cos_sim = np.mean(m, axis=0) \n",
    "    else: \n",
    "        cos_sim = m\n",
    "    index = np.argsort(cos_sim)[::-1] # from highest idx to smallest score \n",
    "    if mask is not None:\n",
    "        assert mask.shape == m.shape\n",
    "        mask = mask[index]\n",
    "    else:\n",
    "        mask = np.ones(len(cos_sim))\n",
    "    mask = np.logical_or(cos_sim[index] != 0, mask) #eliminate 0 cosine distance\n",
    "    best_index = index[mask][:topk]  \n",
    "    return best_index\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Adapt stop words\n",
    "token_stop = tokenizer(' '.join(stop_words), lemmatize=False)\n",
    "\n",
    "# Fit TFIDF\n",
    "vectorizer = TfidfVectorizer(stop_words=token_stop, tokenizer=tokenizer) \n",
    "tfidf_mat = vectorizer.fit_transform(post_1_df['Data'].values) # -> (num_sentences, num_vocabulary)\n",
    "# tfidf_mat.shape\n",
    "\n",
    "def get_recommendations_tfidf(sentence):\n",
    "    \n",
    "    \"\"\"\n",
    "    Return the database sentences in order of highest cosine similarity relatively to each \n",
    "    token of the target sentence. \n",
    "    \"\"\"\n",
    "    # Embed the query sentence\n",
    "    tokens = [str(tok) for tok in tokenizer(sentence)]\n",
    "    vec = vectorizer.transform(tokens)\n",
    "    # Create list with similarity between query and dataset\n",
    "    mat = cosine_similarity(vec, tfidf_mat)\n",
    "    # Best cosine distance for each token independantly\n",
    "    print(mat.shape)\n",
    "    best_index = extract_best_indices(mat, topk=10)\n",
    "    return best_index\n",
    "\n",
    "def search_tf(query):\n",
    "    best_index = get_recommendations_tfidf(query)\n",
    "    #l=[]\n",
    "    df=pd.DataFrame(post_1_df[['Post Title']].iloc[best_index])\n",
    "    df['post_id']=best_index\n",
    "    l=df.values.tolist()\n",
    "    return l\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route(\"/\")\n",
    "def home():\n",
    "    return render_template('home.html')\n",
    "\n",
    "@app.route(\"/recommend\")\n",
    "def recommend():\n",
    "    movie = request.args.get('movie')\n",
    "    r = get_recommendations_tfidf(movie)\n",
    "#     movie = movie.upper()\n",
    "    if type(r)==type('string'):\n",
    "        return render_template('recommend.html',movie=movie,r=r)\n",
    "    else:\n",
    "        return render_template('recommend.html',movie=movie,r=r)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "german-industry",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
